{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "`langchain_community`  \n",
    "`langchain-huggingface`  \n",
    "`langchain-openai`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Basic Embedding Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=1024  # size of the embeddings you want returned.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = embeddings.embed_query(\"HOME0001 is a peer-to-peer housing collective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_query))\n",
    "print(len(test_query))\n",
    "print(test_query[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed list of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\n",
    "        \"0001 HOMES ARE FULLY EQUIPPED, PART OF A GLOBAL NETWORK, AND UNIQUELY SIMPLE TO BUY AND OWN.\",\n",
    "        \"WE WORK WITH WORLD RENOWNED ARCHITECTS TO DESIGN FULLY FURNISHED HOMES THAT ARE READY FROM DAY ONE.\",\n",
    "        \"EACH 0001 HOME IS PART OF OUR GLOBAL PEER-TO-PEER HOUSING COLLECTIVE.\",\n",
    "        \"0001 MEMBERS HELP SHAPE OUR COLLECTIVE AND CAN STAY FOR FREE IN ANY OF OUR LOCATIONS AROUND THE WORLD.\",\n",
    "        \"WE’VE REINVENTED THE HOME BUYING EXPERIENCE SO YOU CAN PURCHASE OUR HOMES SECURELY, ONLINE, IN MINUTES.\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = embeddings.embed_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedded_docs), len(embedded_docs[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create simple Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document Objects\n",
    "from langchain.schema import Document\n",
    "prepped_documents = [Document(page_content=text) for text in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(prepped_documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a similarity search with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"is furniture included?\"\n",
    "\n",
    "results = vector_store.similarity_search(query=query, k=1)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"* {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly advanced example with LLM integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from a URL  \n",
    "https://python.langchain.com/docs/integrations/document_loaders/web_base/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocumentLoaders are objects that load in data from a source and return a list of Documents.  \n",
    "A Document is an object with some page_content (str) and metadata (dict).  \n",
    "https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import os\n",
    "\n",
    "os.environ['USER_AGENT'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a single page  \n",
    "NOTE: our website currently doesn't seem to work well w/ the WebBaseLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.home0001.com/how-it-works\")\n",
    "doc = loader.load()\n",
    "print(doc[0].page_content[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_multiple_pages = WebBaseLoader([\"https://www.home0001.com/how-it-works\", \"https://www.home0001.com/legal\"])\n",
    "docs = loader_multiple_pages.load()\n",
    "print(docs[1].page_content[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk, split and store the data\n",
    "\n",
    "-> it's important to figure out the right chunk size later on\n",
    "\n",
    "We use RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size.  \n",
    "This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.  \n",
    "\n",
    "Next we need to index our text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# set up the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# split the docs\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(splits))\n",
    "print(len(splits[1].page_content))\n",
    "print(splits[1].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vector database\n",
    "\n",
    "In this example we use `OpenAIEmbeddings` and a `Chroma` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector database with the splits\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
    "    # persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Retriever is an interface that returns relevant Documents from an index based on a string query.  \n",
    "\n",
    "A vector store retriever is a retriever that uses a vector store to retrieve documents.  \n",
    "\n",
    "Any VectorStore can easily be turned into a Retriever with `VectorStore.as_retriever()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What is home0001?\")\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other retrieval techniques include:  \n",
    "- MultiQueryRetriever generates variants of the input question to improve retrieval hit rate.\n",
    "- MultiVectorRetriever instead generates variants of the embeddings, also in order to improve retrieval hit rate.\n",
    "- Maximal marginal relevance selects for relevance and diversity among the retrieved documents to avoid passing in duplicate context.\n",
    "- Documents can be filtered during vector store retrieval using metadata filters, such as with a Self Query Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# use default prompt template\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "# print(example_messages)\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# print(format_docs(docs))\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Home0001?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print('\\n')\n",
    "print(rag_chain.invoke(\"can i rent an apartment?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More detailed examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents with FireCrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "fc_api_key = os.getenv('FIRECRAWL_API_KEY')\n",
    "\n",
    "from langchain_community.document_loaders.firecrawl import FireCrawlLoader\n",
    "\n",
    "loader = FireCrawlLoader(\n",
    "    api_key=fc_api_key, url=\"https://www.home0001.com/\", mode=\"crawl\"\n",
    ")\n",
    "\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0].page_content[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various other embedding models to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"gpt4all[cuda]\"\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "hf_embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "gpt4all_embd = GPT4AllEmbeddings()\n",
    "# does this even make sense? \n",
    "# ollama embeddings are huge, slow and not great\n",
    "ollama_embd = OllamaEmbeddings(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BGE models on the HuggingFace are one of the best open-source embedding models. BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "bge_embd = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to pass query_instruction=\"\" for model_name=\"BAAI/bge-m3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up various text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "char_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0\n",
    "    )\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    separators=[\" \", \",\", \"\\n\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of default or recursive, we can also use a Semantic Chunker with one of our Embedding Models.  \n",
    "\n",
    "https://python.langchain.com/docs/how_to/semantic-chunker/  \n",
    "\n",
    "The default way to split is based on `percentile`. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split.  \n",
    "there are also options like `standard_deviation`, `interquartile` or `gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "hf_sem_text_splitter = SemanticChunker(hf_embd, breakpoint_threshold_type=\"percentile\")\n",
    "llama_sem_text_splitter = SemanticChunker(ollama_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splits = char_text_splitter.split_documents(pages)\n",
    "rec_splits = rec_text_splitter.split_documents(pages)\n",
    "hf_sem_splits = hf_sem_text_splitter.split_documents(pages)\n",
    "llama_sem_splits = llama_sem_text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(char_splits), len(char_splits[0].page_content))\n",
    "print(len(rec_splits), len(rec_splits[0].page_content))\n",
    "print(len(hf_sem_splits), len(hf_sem_splits[0].page_content))\n",
    "print(len(llama_sem_splits),len(llama_sem_splits[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vector Databases from the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain_community faiss-cpu\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_vectorstore_hf = FAISS.from_documents(hf_sem_splits, hf_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Chroma is being a bit picky about metadata formats, this should solve it\n",
    "cleaned_splits = filter_complex_metadata(llama_sem_splits)\n",
    "\n",
    "chroma_vectorstore_llama = Chroma.from_documents(\n",
    "    cleaned_splits, \n",
    "    hf_embd\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW When using FAISS it's possible to merge -> look at multi-bot example in rag-bots\n",
    "\n",
    "\n",
    "`db1 = FAISS.from_texts([\"foo\"], embeddings)`  \n",
    "`db2 = FAISS.from_texts([\"bar\"], embeddings)`   \n",
    "`db1.merge_from(db2)`\n",
    "\n",
    "ALSO possible to use several retrievers:\n",
    "https://python.langchain.com/docs/how_to/ensemble_retriever/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_hf_retriever = faiss_vectorstore_hf.as_retriever()\n",
    "\n",
    "chroma_llama_retriever = chroma_vectorstore_llama.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Where is home0001 available?\"\n",
    "\n",
    "print(faiss_hf_retriever.invoke(test_query))\n",
    "print(chroma_llama_retriever.invoke(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search (`mmr`), you can specify that as the search type.  \n",
    "\n",
    "We can also set a similarity `score_threshold` and only return documents with a score above that threshold, as well as top `k` documents returned by the retriever.  \n",
    "\n",
    "`MultiQueryRetriever` generates variants of the input question to improve retrieval hit rate.  \n",
    "`MultiVectorRetriever` instead generates variants of the embeddings, also in order to improve retrieval hit rate.  \n",
    "`Maximal marginal relevance` selects for relevance and diversity among the retrieved documents to avoid passing in duplicate context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are various ways of creating the retriever:\n",
    "sim_retriever = chroma_vectorstore_llama.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5, \"k\": 1}\n",
    ")\n",
    "mmr_retriever = chroma_vectorstore_llama.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple version\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly more verbose\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": faiss_hf_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_chain.invoke(\"How do I book a 0001 home somewhere else? \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
