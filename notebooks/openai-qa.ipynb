{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new assistant with `file_search` enabled in the `tools` parameter of the Assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI()\n",
    "\n",
    "instructions = ''' \n",
    "You are an expert customer service rep for the housing collective HOME0001. Use your knowledge base to answer questions about the project. \n",
    "If you don't find an answer just say 'I don't know :('. Only answer questions related to the project.\n",
    "Talk in a casual, pragmatic tone. Avoid marketing or corporate speak at all costs\",\n",
    "'''\n",
    " \n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"HOME0001 Customer Assistant\",\n",
    "  instructions=instructions,\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload files and add the to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store\n",
    "vector_store = client.beta.vector_stores.create(name=\"FAQ\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"data/home0001qa.json\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update assistant to use the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file_search tool implements several retrieval best practices out of the box to help you extract the right data from your files and augment the modelâ€™s responses.  \n",
    "The file_search tool:\n",
    "\n",
    "- Rewrites user queries to optimize them for search.\n",
    "- Breaks down complex user queries into multiple searches it can run in parallel.\n",
    "- Runs both keyword and semantic searches across both assistant and thread vector stores.\n",
    "- Reranks search results to pick the most relevant ones before generating the final response.\n",
    "\n",
    "By default, the file_search tool uses the following settings but these can be configured to suit your needs:\n",
    "\n",
    "- Chunk size: 800 tokens\n",
    "- Chunk overlap: 400 tokens\n",
    "- Embedding model: text-embedding-3-large at 256 dimensions\n",
    "- Maximum number of chunks added to context: 20 (could be fewer)\n",
    "- Ranker: auto (OpenAI will choose which ranker to use)\n",
    "- Score threshold: 0 minimum ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"are u a communist?\"\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a run and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just here to help with questions about HOME0001, the housing collective, not to dive into political ideologies. If you have any questions about the project, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "\n",
    "print(message_content.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
