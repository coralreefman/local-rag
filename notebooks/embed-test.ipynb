{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "def prepare_qa_documents(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        qa_data = json.load(f)\n",
    "    \n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=item[\"answer\"],\n",
    "            metadata={\"question\": item[\"question\"]}\n",
    "        )\n",
    "        for item in qa_data\n",
    "    ]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "test_documents = prepare_qa_documents(\"../data/home0001qa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q: Do i own my 0001 home outright? A: When you buy a 0001 home, you own the title in the traditional way. If you need, weâ€™ll help you find the right mortgage and can recommend real estate lawyers. You keep full legal ownership of your home, with the added benefit that you can spend time in other locations whenever you want.']\n"
     ]
    }
   ],
   "source": [
    "def prepare_qa_texts(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = [f\"Q: {item['question']} A: {item['answer']}\" for item in data]\n",
    "    \n",
    "    return texts\n",
    "\n",
    "test_texts = prepare_qa_texts(\"../data/home0001qa.json\")\n",
    "print(test_texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=1024  # size of the embeddings you want returned.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def get_hf_embeddings(model_name):\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "mpnet_embeddings = get_hf_embeddings(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "def get_bge_embeddings(model_name, model_kwargs, encode_kwargs):\n",
    "\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name, \n",
    "        model_kwargs=model_kwargs, \n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "bge_embeddings = get_bge_embeddings(model_name, model_kwargs, encode_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomic\n",
    "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: the text prompt must include a task instruction prefix, instructing the model which task is being performed.  \n",
    "\n",
    "For example, if you are implementing a RAG application, you embed your documents as search_document: <text here> and embed your user queries as search_query: <text here>.  \n",
    "\n",
    "Embed texts as documents:  \n",
    "`documents = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']`  \n",
    "\n",
    "Embed texts as queries:  \n",
    "`queries = ['search_query: Who is Laurens van Der Maaten?']`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code':True}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "nomic_embeddings = get_bge_embeddings(\n",
    "    model_name, \n",
    "    model_kwargs, \n",
    "    encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "nomic_embeddings_alt = NomicEmbeddings(model='nomic-embed-text-v1.5', inference_mode='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stella\n",
    "https://huggingface.co/dunzhang/stella_en_1.5B_v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models have multiple dimensions: 512, 768, 1024, 2048, 4096, 6144 and 8192.\n",
    "\n",
    "The higher the dimension, the better the performance. Generally speaking, 1024d is good enough. The MTEB score of 1024d is only 0.001 lower than 8192d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Queries need one out of two prompts. \"s2p_query\" (e.g. retrieve task) and \"s2s_query\" (e.g. semantic textual similarity task) for sentence-to-passage and sentence-to-sentence tasks, respectively.  \n",
    "Documents don't need prompts.\n",
    "\n",
    "Prompt of s2p task(e.g. retrieve task):\n",
    "\n",
    "`Instruct: Given a web search query, retrieve relevant passages that answer the query.\\nQuery: {query}`  \n",
    "\n",
    "Prompt of s2s task(e.g. semantic textual similarity task):  \n",
    "\n",
    "`Instruct: Retrieve semantically similar text.\\nQuery: {query}`  \n",
    "\n",
    "\n",
    "They are defined in `config_sentence_transformers.json`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE ###\n",
    "query_prompt_name = \"s2p_query\"\n",
    "queries = [\n",
    "    \"What are some ways to reduce stress?\",\n",
    "    \"What are the benefits of drinking green tea?\",\n",
    "]\n",
    "query_embeddings = model.encode(queries, prompt_name=query_prompt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code':True}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "stella_15_embeddings = get_bge_embeddings(\n",
    "    model_name,\n",
    "    model_kwargs, \n",
    "    encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code':True}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "stella_400_embeddings = get_bge_embeddings(\n",
    "    model_name,\n",
    "    model_kwargs, \n",
    "    encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "cohere_embeddings = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "mistral_embeddings = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def basic_retriever(documents, embeddings):\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = {\n",
    "    \"openai\": openai_embeddings,\n",
    "    \"mpnet\": mpnet_embeddings,\n",
    "    \"bge\": bge_embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['openai', 'mpnet', 'bge'])\n"
     ]
    }
   ],
   "source": [
    "print(test_embeddings.keys())\n",
    "\n",
    "for model in test_embeddings:\n",
    "\n",
    "    vectorstore = FAISS.from_documents(test_documents, test_embeddings[model])\n",
    "    vectorstore.save_local(\"./FAISS\", model)\n",
    "    # retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_retrievers = []\n",
    "\n",
    "for model in test_embeddings:\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"./FAISS\", \n",
    "        embeddings=test_embeddings[model], \n",
    "        index_name=model, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 4}\n",
    "    )\n",
    "    test_retrievers.append(retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'question': 'Who is behind home0001?'}, page_content='Home0001 is initiated by a multi-disciplinary collective working across art, architecture, technology, and design, and currently based in los angeles, new york, paris, berlin, and london. Designed together with world renowned architects, 0001 homes are fully equipped and furnished and are part of an expanding network.'),\n",
       " Document(metadata={'question': 'Can i change the design of my home?'}, page_content=\"Legally you own your home and are free to do what you want with it. However, to maintain access to home0001's network in other locations, your home does need to meet our standards and our team can support you in making changes where desired.\"),\n",
       " Document(metadata={'question': 'How do i book an 0001 home somewhere else?'}, page_content=\"Whenever you want to spend time in other home0001 locations, just text us your dates and we'll confirm availability right away. You cover one cleaning fee each time you swap homes and don't pay any nightly rates. 0001 homes are fitted with smart locks, so you can gain access to your new place with your phone. Youâ€™ll be invited to any community events that are taking place locally and youâ€™ll have continual access to our virtual concierge service so you can text us whenever you need anything.\"),\n",
       " Document(metadata={'question': 'What is home0001?'}, page_content='Home0001 is a global housing network. Each 0001 home is fully-equipped and furnished. Move in with just your suitcase. Swap cities whenever you like.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_retrievers[0].invoke(\"what is home0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'question': 'What are the perks of joining the home0001 network?'}, page_content='Home0001 is a distributed housing collective: in addition to community dinners and events, homeowners get access to 0001 homes in other cities for free. No nightly rate; just a cleaning fee each time. Own one home, live in many places. '),\n",
       " Document(metadata={'question': 'Are 0001 homes move-in ready?'}, page_content='Developed in collaboration with world-renowned architects, every single thing in an 0001 home is thoughtfully designed with a focus on simplicity and functionality, so homebuyers can literally move in with nothing but their suitcase.'),\n",
       " Document(metadata={'question': 'Can i change the design of my home?'}, page_content=\"Legally you own your home and are free to do what you want with it. However, to maintain access to home0001's network in other locations, your home does need to meet our standards and our team can support you in making changes where desired.\"),\n",
       " Document(metadata={'question': 'Who founded home0001?'}, page_content='Home0001 is a new form of housing initiated by a collective of architects, artists, technologists, and designers currently based in los angeles, new york, paris, rotterdam, berlin, and london.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_retrievers[1].invoke(\"what is home0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'question': 'How does the home0001 network function?'}, page_content='Home0001 is a distributed housing collective: in addition to community dinners and events, homeowners get access to 0001 homes in other cities for free. No nightly rate; just a cleaning fee each time. Own one home; live flexibly between multiple locations.'),\n",
       " Document(metadata={'question': 'Who founded home0001?'}, page_content='Home0001 is a new form of housing initiated by a collective of architects, artists, technologists, and designers currently based in los angeles, new york, paris, rotterdam, berlin, and london.'),\n",
       " Document(metadata={'question': 'What is home0001?'}, page_content='Home0001 is a global housing network. Each 0001 home is fully-equipped and furnished. Move in with just your suitcase. Swap cities whenever you like.'),\n",
       " Document(metadata={'question': 'Can i buy a home as a non-us citizen?'}, page_content=\"The process for buying an 0001 home is the same wherever youâ€™re from. There's no extra stamp duty, taxes, or any other additional costs for non-us citizens.\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_retrievers[2].invoke(\"what is home0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_PROMPT = \"\"\"\n",
    "Expected Response: {expected_response}\n",
    "Actual Response: {actual_response}\n",
    "---\n",
    "(Answer with 'true' or 'false') Does the actual response match the expected response? \n",
    "\"\"\"\n",
    "\n",
    "def test_ticket_to_ride_rules():\n",
    "    assert query_and_validate(\n",
    "        question=\"How many points does the longest continuous train get in Ticket to Ride? (Answer with the number only)\",\n",
    "        expected_response=\"10 points\",\n",
    "    )\n",
    "\n",
    "\n",
    "def query_and_validate(question: str, expected_response: str):\n",
    "    response_text = query_rag(question)\n",
    "    prompt = EVAL_PROMPT.format(\n",
    "        expected_response=expected_response, actual_response=response_text\n",
    "    )\n",
    "\n",
    "    # model = Ollama(model=\"mistral\")\n",
    "    evaluation_results_str = model.invoke(prompt)\n",
    "    evaluation_results_str_cleaned = evaluation_results_str.strip().lower()\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    if \"true\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Green if it is correct.\n",
    "        print(\"\\033[92m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return True\n",
    "    elif \"false\" in evaluation_results_str_cleaned:\n",
    "        # Print response in Red if it is incorrect.\n",
    "        print(\"\\033[91m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid evaluation result. Cannot determine if 'true' or 'false'.\"\n",
    "        )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
